\chapter{Initial work}
\section{Prony's method}

\section{Univariate case: adaptation of Prony's method}
The initial setting is as follows. We construct two $m\times m$ complex valued Hankel matrices $H_0$ and $H_1$, that is, matrices such that their skew-diagonals\footnote{A skew-diagonal is the diagonal in the North-East direction.} are constants. We have $D_w\in\mathbb{C}^{k\times k}$ where $[D_w]_{j,j}=w_j$ and $D_\mu\in\mathbb{C}^{k\times k}$ where $[D_\mu]_{j,j}=e^{i\pi\mu^{(j)}}$. We furthermore have a Vandermonde matrix $V_m\in\mathbb{C}^{m\times k}$ defined as $$V_m=
\begin{pmatrix}
    1&\ldots&1\\
    \big(e^{i\pi\mu^{(1)}}\big)^1&\ldots&\big(e^{i\pi\mu^{(k)}}\big)^1\\
    \vdots&\ldots&\vdots\\
    \big(e^{i\pi\mu^{(1)}}\big)^{m-1}&\ldots&\big(e^{i\pi\mu^{(k)}}\big)^{m-1}
\end{pmatrix}$$ We construct a 3rd order tensor $F\in\mathbb{C}^{m\times m\times 2}$ in which $F_{i,i',j}=[H_{j-1}]_{i,i'}$ for $j=1,2$ and $i,i'\in[m]$.
\begin{fact}
    In the setting above, $F$ admits the unique rank $k$ tensor decomposition $F=V_m\otimes V_m\otimes (V_2D_w)$.
\end{fact}
\begin{proof}
    We start by computing the product $V_2D_w$. By definition of $V_m$, we have that $[V_2]_{r,c}=\big(e^{i\pi\mu^{(c)}}\big)^{r-1}$ for $r\in[2]$ and $c\in[k]$. Multiplying with the diagonal matrix $D_w$ yields $$[V_2D_w]_{r,c}=w_c\big(e^{i\pi\mu^{(c)}}\big)^{r-1},\quad\forall r\in[2],c\in[k].$$
    We aim to show that 
    \begin{flalign*}
        F_{i_1,i_2,i_3}&=\sum_{n=1}^k[V_m]_{i_1,n}[V_m]_{i_2,n}[V_2D_w]_{i_3,n}\\
        &=\sum_{n=1}^k\big(e^{i\pi\mu^{(n)}}\big)^{i_1-1}\big(e^{i\pi\mu^{(n)}}\big)^{i_2-1}w_n\big(e^{i\pi\mu^{(n)}}\big)^{i_3-1}\\
        &=\sum_{n=1}^k w_n\big(e^{i\pi\mu^{(n)}}\big)^{i_1+i_2+i_3-3},
    \end{flalign*} for $i_1,i_2\in[m]$ and $i_3\in[2]$. It can be verified that the measurements that form both Hankel matrices are given by $$f(s)=\sum_{j\in[k]}w_j\big(e^{i\pi\mu^{(j)}}\big)^s,$$ with $s=0,\ldots,2m-1$ for $H_0$ and $s=1,\ldots,2m$ for $H_1$. For the details of the proof, see Appendix A. Then, since $F_{i_1,i_2,i_3}=[H_{i_3-1}]_{i_1,i_2}$ by definition of $F$ for $i_1,i_2\in[m]$ and $i_3\in[2]$, we directly get that 
    $$F_{i_1,i_2,i_3}=\sum_{n=1}^k w_n\big(e^{i\pi\mu^{(n)}}\big)^{i_1+i_2+i_3-3},$$ as required.
\end{proof}
\section{Multivariate toy case}
We follow the definitions given in the paper. We have $D_w=Diag(w_j)\in\mathbb{C}^{k\times k}$ and $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}\in\mathbb{C}^{d\times k}$ the latter in which $\mu_n^{(i)}$ are i.i.d. $\sim Unif([-1,+1])$. Furthermore, we have $F_{n_1,n_2,n_3}=f(s)\big|_{s=e_{n_1}+e_{n_2}+e_{n_3}},$ for all $n_1,n_2,n_3\in[d]$. We have the following fact:
\begin{fact}
    In the setting above, $F$ admits the tensor decomposition $F=V_d\otimes V_d\otimes(V_dD_w)$.
\end{fact}
\begin{proof}
    We wish to show that $f(e_1+e_2+e_3)=\sum_{j=1}^k w_je^{i\pi(\mu_1^{(j)}+\mu_2^{(j)}+\mu_3^{(j)})}$. To do so, we first compute the matrix product $V_dD_w$. Since $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}$ for $r\in[d],c\in[k]$ and $D_w=Diag(w_j)$, we directly have that $$[V_dD_w]_{r,c}=w_ce^{i\pi\mu_r^{(c)}},\quad r\in[d],c\in[k],$$ so that by definition of tensor decomposition, we get
    \begin{flalign*}
        F_{n_1,n_2,n_3}&=\sum_{j=1}^k[V_d]_{n_1,j}[V_d]_{n_2,j}[V_dD_w]_{n_3,j}\\
        &=\sum_{j=1}^k e^{i\pi\mu_{n_1}^{(j)}}e^{i\pi\mu_{n_2}^{(j)}}w_je^{i\pi\mu_{n_3}^{(j)}}\\
        &=\sum_{j=1}^k w_je^{i\pi(\mu_{n_1}^{(j)}+\mu_{n_2}^{(j)}+\mu_{n_3}^{(j)})}=f(e_1+e_2+e_3),
    \end{flalign*} as required.
\end{proof}