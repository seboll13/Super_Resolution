\chapter{Discussion}
\section{Improvements already made}
The algorithm proposed in the previous chapter allows us to efficiently recover the position of some image source points using only a polynomial amount of space. Recall that the problem instance has multiple base parameters:
\begin{enumerate}
    \item the number of dimensions $d$ needed to represent the entirety of the fine details of the image,
    \item the number of point sources $k$ we wish to recover,
    \item the minimal distance $\Delta$ between any two pair of points on the plane in the $L_2$ sense,
    \item the number $m$ of coarse measurements we have to make in order to efficiently recover the point sources in the noisy case.
\end{enumerate}
The cutoff frequency $R$ of the measurements is in this setting inversely proportional to the minimum distance $\Delta$, that is, $R=O(1/\Delta)$. Compared to previous work which assumed that $R$ could be as large as $\Omega(\sqrt{d}/\Delta)$, data can now be super-resolved well enough with a pretty small cutoff frequency and hence induces a robust enough procedure. Moreover, the number of measurements needed for the procedure to work in a stable enough way do not depend on the distance between the points anymore, but only depend on the number of points and the overall dimension. Indeed previous results required an exponential number of measurements to be taken on the hypergrid, whereas the current procedure's computational complexity is bounded by a polynomial in both $d$ and $k$. This hence results in an exponential improvement.
\section{Stability and efficiency}
To complement what has been stated in previous sections, we reiterate on the fact that stability guarantees of the procedure depend on one element: the condition number $\cond(V)$ of the random Vandermonde matrix. Since the measurements are stored in a tensor, we seek the stability of the tensor decomposition procedure. For it to be stable, we need $\cond(V)$ to be close to 1 in expectation; this happens if the measurements follow a Gaussian distribution.
\paragraph{Efficiency of Vandermonde matrices} Many studies have already shown that Vandermonde matrices tend to be badly ill-conditioned in most scenarios, with the exception of some specific cases such as DFT matrices or as it turns out, Gaussian valued matrices \cite{vandermondeMatrices}.\par 
Recall that the condition number of a matrix governs its tolerance to noisy data. Specifically, it measures how sensitive a particular matrix is to perturbations that can appear in the input data. Although an ill-conditioned matrix augments the sensitivity of its inverse, the eigenvalue problem can stay well conditioned. In other words, it can happen that a matrix is poorly conditioned for inversion whilst the eigenvalue problem is well conditioned, or vice versa. Hence, focusing on the condition number of the measurement matrix is a key element to not neglect whilst wanting to improve the sample complexity of the procedure.
\section{Improvements left to make}
The open problem related to this algorithm is to reduce the sample complexity from quadratic to linear. Namely, we seek a reduction of the number measurements made, from $O(2(m')^2)$ to $O(m')$. Recall that $m'=m+d+1$ corresponds to the first two dimensions of our measurement tensor, $m$ of which are Gaussian, the others respectively being the $d$ basis vectors needed to recover the location of the point sources and a row of ones used for normalisation purposes. If we wish to take a linear amount of measurements, the procedure to generate them will have to change, to wit, we remove the convolution of the two Gaussians. 