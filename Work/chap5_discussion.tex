\chapter{Discussion}
\section{Improvements already made}
The algorithm proposed in the previous chapter allows us to efficiently recover the position of some image source points using only a polynomial amount of space. Recall that the problem instance has multiple base parameters:
\begin{enumerate}
    \item the number of dimensions $d$ needed to represent the entirety of the fine details of the image,
    \item the number of point sources $k$ we wish to recover,
    \item the minimal distance $\Delta=\min_{j'\neq j}||\widehat{\mu}^{(j')}-\widehat{\mu}^{(j)}||_2$ between any two pair of points $j$ and $j'$ on the $\mathbb{R}^d$ plane, and
    \item the number $m$ of coarse measurements we have to make in order to efficiently recover the point sources in the noisy case.
\end{enumerate}
The cutoff frequency $R$ of the measurements is in this setting inversely proportional to the minimum distance $\Delta$, that is, $R=O(1/\Delta)$. Compared to previous work which assumed that $R$ could be as large as $\Omega(\sqrt{d}/\Delta)$, data can now be super-resolved well enough with a pretty small cutoff frequency and hence induces a robust enough procedure. Moreover, the number of measurements needed for the procedure to work in a stable enough way do not depend on the distance between the points anymore, but only depend on the number of points and the overall dimension. Indeed previous results required an exponential number of measurements to be taken on the hypergrid, whereas the current procedure's computational complexity is bounded by a polynomial in both $d$ and $k$. This hence results in an exponential improvement.

\section{Robustness of a random Vandermonde matrix}
To complement what has been stated in previous sections, we reiterate on the fact that stability guarantees of the procedure depend on one major factor: the condition number $\cond(V_S)$ of the random Vandermonde matrix that contains our measurements. Since those are stored in a tensor, we seek the stability of the tensor decomposition procedure. For it to be stable, we need $\cond(V_S)$ to be close to 1 in expectation; this happens if the measurements are sampled from a Gaussian distribution. Again, since a single measurement is made from the convolution of two Gaussians, it allows us to express the product $V_S^*V_S$ as the sample mean of random Hermitian matrices the spectrum of which is close enough to that of their average in expectation. From there, noting that each measurement lies on the unit circle of the complex plane, a Matrix-Hoeffding bound suffices to conclude on a close bound for $\cond(V_S)$ that holds with high probability.\par

Since our measurements are stored in a Vandermonde matrix, it is important to observe the behaviour of their condition number. Recent studies have shown that such matrices tend to be badly ill-conditioned in most scenarios, with the exception of some specific cases such as DFT matrices or as it turns out, complex Gaussian valued matrices \cite{vandermondeMatrices}. The bounds in question get better when the randomly generated points that form the Vandermonde matrix lie closer to the unit circle of the complex plane. In our case, the measurements indeed lie on the complex circle and thus provide a strong stability guarantee of recovery.\par

Recall that the condition number of a matrix governs its tolerance to noisy data. Specifically, it measures how sensitive a particular matrix is to perturbations that can appear in the input data. Although an ill-conditioned matrix augments the sensitivity of its inverse, the eigenvalue problem can stay well conditioned. In other words, it can happen that a matrix is poorly conditioned for inversion whilst the eigenvalue problem is well conditioned, or vice versa. Hence, keeping an eye on the condition number of the measurement matrix is a key element to not neglect whilst wanting to improve the sample complexity of the procedure.

\section{Improvements left to be made}
The open problem related to this algorithm is to reduce the sample complexity from quadratic to linear. Namely, we seek a reduction of the number of measurements made, from $O(k^2)$ to $O(k)$. Recall that $m'=m+d+1=\widetilde{O}(k)$ corresponds to the first two dimensions of our measurement tensor, $m$ of which are Gaussian, the others respectively being the $d$ basis vectors needed to recover the location of the point sources and a row of ones used for normalisation purposes. If we wish to take a linear amount of measurements, the procedure to generate them will have to change, to wit, we remove the convolution of the two Gaussians, consider points on the real line and form a collection of measurements that we store in a tensor. This however reduces to the 1D matrix-pencil method introduced in chapter 3.\par

Another important information to take into consideration is the assumption of the minimal distance $\Delta$ between the points. The intuition behind this is that the closer the source points are, the harder it gets to recover them efficiently. In the main algorithm, $\Delta$ is inversely proportional to the cutoff frequency $R$ of the measurements. That is, if $\Delta$ is small, $R$ will be large and thus the frequency domain won't give us precise enough information concerning the concentration of the point sources on the plane. However, we will get a smoother result if $\Delta$ grows large. Hence, a way to still have a robust procedure when the point sources are close to each other is to find a way to decorrelate $\Delta$ and $R$ so that we can still recover good enough estimates of the point sources when they are closely concentrated.\par

To conclude, there exists multiple ways to improve the general recovery procedure, the majority of which rely on the measurements of the frequency domain. The robustness of the recovery relies on the condition number of the measurement matrices but also on the way we perform the measurements. In essence, improving the main procedure will most likely involve either generating the measurements differently, or imposing more assumptions on the multiple parameters we face.