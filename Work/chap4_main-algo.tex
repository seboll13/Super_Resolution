\chapter{Main algorithm}
Let us recall what the goal of the problem is. Broadly speaking, super-resolution aims to recover a superposition of point sources using bandlimited measurements that may be corrupted with noise. In this setting, the created algorithm works in the Fourier domain where each of the $k$ points of the $d$-dimensional plane are separated by a distance at least $\Delta$. Hence, the frequencies of the Fourier measurements are bounded by $O(1/\Delta)$. The idea of the procedure is to generate a sample of random bandlimited measurements such that the estimates of the point sources are precise enough.\par 
This chapter discusses the main algorithm along with its analysis. We will go through all the important results and describe in detail their implication on the main procedure to effectively recover the point sources.

\section{Algorithm description}
\subsection{Introduction}
The basic idea of the algorithm is to efficiently solve the problem of recovering the superposition of our point sources using \textit{coarse} Fourier measurements. Once again, we are given a signal $x(t)=\sum_{j\in[k]}w_j\delta_{\mu^{(j)}}$ from which we want to recover the $w_j$ and $\mu^{(j)}$ coefficients. We have access to a noisy measurement function $\tilde{f}(\cdot)$. The idea is to generate random samples $s$ and evaluate $\tilde{f}$ for each of them in order to find the best fit, i.e. the best fitting $\mu^{(j)}$'s. More formally, for all $s$ we compute $$\tilde{f}(s)=\sum_{j\in[k]}w_je^{i\pi\langle\mu^{(j)},s\rangle}+z(s),$$ where $z(s)$ is the noise upper bounded by $\epsilon_z$ for every $s$ and store those results in a 3-way tensor. Compared to previous work, the algorithm guarantees a certain stability of recovery and runs in quadratic time. Namely, the measurements are bounded in frequency by $O(1/\Delta)$ and the algorithm runs in time $O((k\log k+d)^2)$. The key recovery guarantee relies on the condition number of the random Vandermonde matrix. This is however further discussed in the next section.
\subsection{Main result}
\paragraph{Input and output} The algorithm takes as input a cutoff frequency $R$, the defined number of measurements $m$ and a noisy measurement function $\tilde{f}(\cdot)$ and outputs the set of estimates $$\{\widehat{w}_j,\widehat{\mu}^{(j)}:j\in[k]\},$$ where the $\widehat{w}_j$'s are the complex weight coefficients and the $\widehat{\mu}^{(j)}$'s are the estimates of the point sources. Note that in case the noise is non-existant (i.e. when $\epsilon_z=0$), the parameters are recovered exactly. Otherwise, stable recovery implies that the estimates over all permutations $\pi$ on $[k]$ are such that $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq poly(d,k)\epsilon_z.$$ In words, the estimates of the point sources differ from their real values by at most the noise $\epsilon_z$ scaled by a polynomial function that depends on the number of dimensions $d$ and the number of point sources $k$. Interestingly, the stability of recovery does not depend on the minimal distance between the point sources, but rather focuses on their concentration on the plane.
\paragraph{Measurements} We generate a set of measurements $\mathcal{S}=\{s^{(1)},\ldots,s^{(m+d+1)}\}$ randomly in which
\begin{enumerate}
    \item the $s^{(1)},\ldots,s^{(m)}$ are $m$ i.i.d. samples from the Gaussian distribution $\mathcal{N}(0,R^2I_{d\times d})$,
    \item $s^{(m+n)}=e_n$ for all $n\in[d]$ and
    \item $s^{(m+d+1)}=0$.
\end{enumerate}
We also let $m'=m+d+1$ for writing simplicity. Note that each measurement is a $d$-dimensional vector, i.e. a point in the $d$-dimensional plane. Therefore, the Gaussian measurements are such that $[s^{(i)}]_n$ are non-correlated 0-mean $R^2$-variance gaussians for every $n\in[d]$. We then take a sample $v$ from the unit sphere and set $v^{(1)}=v$ and $v^{(2)}=2v$. Since again we are in dimension $d$, the sample $v$ is such that $v_1^2+\ldots+v_d^2=1$.
\paragraph{Tensor decomposition} We now construct a tensor $$\tilde{F}\in\mathbb{C}^{m'\times m'\times 3}:\tilde{F}_{n_1,n_2,n_3}=\tilde{f}(s)|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}}$$ containing all the $2(m')^2$ possible combinations of the above measurements. We apply Jennrich's algorithm on $\tilde{F}$ to obtain the estimates $\widehat{V}_{S'}$ and $\widehat{D}_w$. Note that the estimate $\widehat{V}_{S'}$ is normalised so that its last line is all ones, in order to match the definition of $V_{S'}$ (see next section).
\paragraph{Read of estimates} We finish by recovering the real part of the estimates of the point sources by setting $\widehat{\mu}^{(j)}=Real(\log([\widehat{V}_{S'}]_{[m+1:m+d,j]})/(i\pi))$ for every $j\in[k]$ and find the best possible matching coefficients by setting $\widehat{W}=\arg\min_{W\in\mathbb{C}^k}||\widehat{F}-\widehat{V}_{S'}\otimes\widehat{V}_{S'}\otimes \widehat{V}_dD_w||_F$, where $||\cdot||_F$ is the Frobenius norm of $(\cdot)$. Here, observe that by definition $[\widehat{V}_{S'}]_{[m+1:m+d,j]}=[\widehat{V}_d]_{[1:d,j]}$, where $\log(\widehat{V}_d)$ is performed element-wise in $\widehat{V}_d$ to recover the list of all estimates.
\subsection{Why we use tensor decomposition}
Jennrich's algorithm consists of a tensor decomposition that allows us to recover the estimates of the point sources. Namely, given a noisy tensor $\tilde{F}$, it will spit out a decomposition that consists of three matrices, one of which we are most interested in. We denote the following fact:
\begin{fact}
    In the exact case (i.e. when $\epsilon_z=0$), the constructed tensor $F$ admits a rank-$k$ decomposition $$F=V_{S'}\otimes V_{S'}\otimes (V_2D_w).$$
\end{fact}
\begin{proof}
    As usual, we start by computing the product $V_2D_w$. Since $D_w$ is a diagonal matrix, we have $$V_2D_w=
    \begin{pmatrix}
        w_1e^{i\pi\langle\mu^{(1)},v^{(1)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(1)}\rangle}\\
        w_1e^{i\pi\langle\mu^{(1)},v^{(2)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(2)}\rangle}\\
        w_1&\ldots&w_k
    \end{pmatrix}.$$
    Notice that once again, we can think of multiplying the last row of $w$'s with an exponential whose exponent is 0. Then, write $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}=e^{i\pi\langle\mu^{(c)},e_r=s(m+r)\rangle}$. Using this, we can express the $r$'th row and the $c$'th column of $V_{S'}$ as $$[V_{S'}]_{r,c}=e^{i\pi\langle\mu^{(c)},s^{(r)}\rangle},\quad r\in[m'],c\in[k].$$ From there for $n_1,n_2\in[m']$ and $n_3\in[3]$, it follows easily that 
    \begin{flalign*}
        F_{n_1,n_2,n_3}&=\sum_{n\in[k]}[V_{S'}]_{n_1,n}[V_{S'}]_{n_2,n}[V_2D_w]_{n_3,n}\\
        &=\sum_{n\in[k]}e^{i\pi\langle\mu^{(n)}, s^{(n_1)}\rangle}e^{i\pi\langle\mu^{(n)},s^{(n_2)}\rangle}w_ne^{i\pi\langle\mu^{(n)},v^{(n_3)}\rangle}\\
        &=\sum_{n\in[k]}w_ne^{i\pi\left(\langle\mu^{(n)},s^{(n_1)}\rangle+\langle\mu^{(n)},s^{(n_2)}\rangle+\langle\mu^{(n)},v^{(n_3)}\rangle\right)}\\
        &=\sum_{n\in[k]}w_ne^{i\pi\langle\mu^{(n)},s^{(n_1)}+s^{(n_2)}+v^{(n_3)}\rangle}\\
        &=\tilde{f}(s)\big|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}},
    \end{flalign*} as required.
\end{proof}\par 
The above fact demonstrates why we use tensors. Indeed, having access to $\widehat{V}_{S'}$ allows us to recover the estimates of the point sources. The next sections are dedicated to the multiple stability guarantees that makes this algorithm both useful and powerful.

\section{Stability guarantees and definitions}
\paragraph{Stability guarantees} The main guarantee on the stability of the recovery of the point sources is given by $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq C\frac{\sqrt{d}k^5}{\Delta\delta_v}\frac{w_{max}}{w_{min}^2}\left(\frac{1+2\epsilon_x}{1-2\epsilon_x}\right)^{5/2}\epsilon_z,$$ where $C$ is a universal constant such that the bound holds with probability at least $(1-\delta_s)$ over the random sampling of $\mathcal{S}$ and with probability at least $(1-\delta_v)$ over the random projections in Jennrich's algorithm. We will see in the following section that the stability guarantees of the algorithm essentially rely on two important results: the stability of Jennrich's algorithm for tensor decomposition, and the tight variation bound that comes from the choice of the measurements.
\paragraph{Some definitions}
The objects that play an essential role in the main algorithm are the following:
\begin{itemize}
    \item $[V_S]_{r,c}=e^{i\pi\langle\mu^{(c)},s^{(r)}\rangle}$ is the \textit{characteristic matrix}, that is, the matrix that contains all the point spread functions generated by the random Gaussian measurements.
    \item $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}$ is the matrix that contains all the complex valued coefficients that lie on the unit circle of the complex plane. Note that this is due to the uniformity of the $\mu$'s.
    \item $V_{S'}=\big[V_S,\, V_d,\, [1]^k\big]^T$ is the matrix, the coefficients of which we shall try to recover during tensor decomposition.
    \item Finally, define $V_2$ such that $[V_2]_{r,c}=e^{i\pi\langle\mu^{(c)},v^{(r)}\rangle}$ where again we defined $v^{(3)}=0$ for simplicity of notation. Note that the exponent being 0 will yield a last row of only ones.
\end{itemize}
\section{Analysis}
\subsection{Stability of tensor decomposition}
A main factor of stability resides in the tensor decomposition procedure. Since the measurements are linearly independent, the tensor decomposition exists and is unique up to column permutation and rescaling. Furthermore, suppose that $F$ admits the decomposition $F=V\otimes V\otimes (V_2D_w)\in\mathbb{C}^{m\times m\times 3}$ and that $\tilde{F}$ is element-wise close to $F$, that is, each of their elements differ by at most a constant $\epsilon_z$. If $\tilde{F}$ is passed as input to Jennrich's algorithm, it happens that $\widehat{V}$ is close enough to $V$ with high probability. To show this, denote $D_1=diag([V_2]_{1,:}D_w)$ and $D_2=diag([V_2]_{2,:}D_w)$ where $[V_w]_{i,:}$ is the $i$'th row vector of $V_2$. Let $F_1=F(I,I,e_1)$. By previous computations, we know that $F_1=VD_1V^T$. Consider the SVD $F_1=P\Lambda P^T$ and let $U=P^TV$. Define $E=F(P,P,I)$.
\begin{fact}
    In the above setting, $E=F(P,P,I)=U\otimes U\otimes (V_2D_w)\in\mathbb{C}^{k\times k\times 3}$.
\end{fact}
\begin{proof}
    By direct computation, we have 
    \begin{flalign*}
        E_{n_1,n_2,n_3}=F(P,P,I)_{n_1,n_2,n_3}&=\sum_{j_1,j_2\in[m']}\sum_{j_3\in[3]}F_{j_1,j_2,j_3}[P]_{j_1,n_1}[P]_{j_2,n_2}[I]_{j_3,n_3}\\
        &=\sum_{j_1,j_2\in[m']}\sum_{j_3\in[3]}\sum_{n\in[k]}[V]_{j_1,n}[V]_{j_2,n}[V_2D_w]_{j_3,n}[P]_{j_1,n_1}[P]_{j_2,n_2}[I]_{j_3,n_3}\\
        &=\sum_{j_1,j_2\in[m']}\sum_{n\in[k]}[V]_{j_1,n}[P]_{j_1,n_1}[V]_{j_2,n}[P]_{j_2,n_2}[V_2D_w]_{n_3,n}\\
        &=\sum_{j_1,j_2\in[m']}\sum_{n\in[k]}[P^T]_{n_1,j_1}[V]_{j_1,n}[P^T]_{n_2,j_2}[V]_{j_2,n}[V_2D_w]_{n_3,n}\\
        &=\sum_{n\in[k]}[P^TV]_{n_1,n}[P^TV]_{n_2,n}[V_2D_w]_{n_3,n},
    \end{flalign*} which is equivalent to writing that $E=(P^TV)\otimes(P^TV)\otimes(V_2D_w)$ when summing over all possible $n_1,n_2$ and $n_3$'s. This concludes the proof.
\end{proof}\par
Now slice the tensor $E$ in two such that we define $E_1=E(I,I,e_1)$ and $E_2=E(I,I,e_2)$. Note that $E_1=UD_1U^T$ and $E_2=UD_2U^T$. Thus, computing $E_1E_2^{-1}$ yields
\begin{flalign*}
    E_1E_2^{-1}&=UD_1U^T(U^TD_2U)^{-1}=UD_1U^T(U^T)^{-1}D_2^{-1}U^{-1}\\
    &=UD_1D_2^{-1}U^{-1}=UDU^{-1}=M,
\end{flalign*} when letting $D=D_1D_2^{-1}$.
\begin{fact}
    In the exact case, $$D=diag(e^{i\pi\langle\mu^{(j)},v^{(1)}-v^{(2)}\rangle}:j\in[k])$$
\end{fact}
\begin{proof}
    To show this, we first have to note that taking the inverse of a diagonal matrix only requires us to take the reciprocal of the diagonal elements and store them in a resulting matrix which is itself diagonal. Now recall that $[V_2D_w]_{r,c}=w_ce^{i\pi\langle\mu^{(c)},v^{(r)}\rangle}$. We can express $D_1$ and $D_2$ a little differently. Namely, write $[D_1]_{j,j}=w_je^{i\pi\langle\mu^{(j)},v^{(1)}\rangle}$ and $[D_2]_{j,j}=w_je^{i\pi\langle\mu^{(j)},v^{(2)}\rangle}$. Using the above fact allows us to express the inverse of $D_2$ using compact form as $[D_2^{-1}]_{j,j}=\frac{1}{w_j}e^{-i\pi\langle\mu^{(j)},v^{(2)}\rangle}$. Hence, $$[D]_{j,j}=[D_1D_2^{-1}]_{j,j}=e^{i\pi\langle\mu^{(j)},v^{(1)}-v^{(2)}\rangle},$$ as required.
\end{proof}
