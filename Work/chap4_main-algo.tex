\chapter{Main algorithm}
Let us briefly recall what the goal of the problem is. Broadly speaking, super-resolution aims at recovering a collection of point sources using bandlimited measurements that may be corrupted with noise. In this setting, the created algorithm works in the Fourier domain where each of the $k$ points of the $d$-dimensional plane are separated by a distance at least $\Delta$. Hence, the frequencies of the Fourier measurements are bounded by $O(1/\Delta)$. The idea of the procedure is to generate a sample of random bandlimited measurements such that the estimates we recover of the point sources are precise enough.\par 
This chapter discusses the main algorithm along with its analysis. We will go through all the important results and describe in detail their implication on the main procedure to effectively recover the point sources.

\section{Algorithm description}
\subsection{Introduction}
The basic idea of the algorithm is to efficiently solve the problem of recovering the superposition of our point sources using \textit{coarse} Fourier measurements. Once again, we are given a signal $x(t)=\sum_{j\in[k]}w_j\delta_{\mu^{(j)}}$ from which we want to recover the $w_j$ and $\mu^{(j)}$ coefficients. We have access to a noisy measurement function $\widetilde{f}(\cdot)$. The idea is to generate random samples $s$ and evaluate $\widetilde{f}$ for each of them in order to find the best fit, i.e. the best fitting $\mu^{(j)}$'s. More formally, for all $s$ we compute $$\widetilde{f}(s)=\sum_{j\in[k]}w_je^{i\pi\langle\mu^{(j)},s\rangle}+z(s),$$ where $z(s)$ is the noise upper bounded by $\epsilon_z$ for every $s$ and store those results in a 3-way tensor. Compared to previous work, the algorithm guarantees a certain stability of recovery and runs in quadratic time. Namely, the measurements are bounded in frequency by $O(1/\Delta)$ and the algorithm runs in time $O((k\log k+d)^2)$. The key recovery guarantee relies on the condition number of the random Vandermonde matrix. This is however further discussed in the next sections.
\subsection{Definitions}
The following objects are the ones that play an essential role in the main algorithm:
\begin{itemize}
    \item $[V_S]_{r,c}=e^{i\pi\langle\mu^{(c)},s^{(r)}\rangle}$ is the \textit{characteristic matrix}, that is, the matrix that contains all the point spread functions generated by the random Gaussian measurements.
    \item $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}$ is the matrix that contains all the complex valued coefficients that lie on the unit circle of the complex plane. Note that this is again, due to the uniformity of the $\mu$'s.
    \item $V_{S'}=\big[V_S,\, V_d,\, [1]^k\big]^T$ is the matrix that will be returned from the tensor decomposition procedure.
    \item Finally, define $V_2$ such that $[V_2]_{r,c}=e^{i\pi\langle\mu^{(c)},v^{(r)}\rangle}$ where we let $v^{(3)}=0$ for simplicity of notation. Note that the exponent being 0 will yield a last row of only ones, useful for normalisation purposes.
\end{itemize}
\subsection{Main result}
\paragraph{Input and output} The algorithm takes as input a cutoff frequency $R$, the defined number of measurements $m$ and a noisy measurement function $\widetilde{f}(\cdot)$ and outputs the set of estimates $$\{\widehat{w}_j,\widehat{\mu}^{(j)}:j\in[k]\},$$ where the $\widehat{w}_j$'s are the complex weight coefficients and the $\widehat{\mu}^{(j)}$'s are the estimates of the point sources. Note that in case the noise is non-existant (i.e. when $\epsilon_z=0$), the parameters are recovered exactly. Otherwise, stable recovery implies that the estimates over all permutations $\pi$ on $[k]$ are such that $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq poly(d,k)\epsilon_z.$$ In words, in the $L^2$ sense, the estimates of the point sources differ from their real values by at most the noise $\epsilon_z$ scaled by a polynomial function that depends on the number of dimensions $d$ and the number of point sources $k$. Interestingly, the stability of recovery does not depend on the minimal distance between the point sources, but rather focuses on how they are concentrated on the plane.
\paragraph{Measurements} We generate a set of measurements $\mathcal{S}=\{s^{(1)},\ldots,s^{(m+d+1)}\}$ randomly in which
\begin{enumerate}
    \item the $s^{(1)},\ldots,s^{(m)}$ are $m$ i.i.d. samples from the Gaussian distribution $\mathcal{N}(0,R^2I_{d\times d})$,
    \item $s^{(m+n)}=e_n$ for all $n\in[d]$ and
    \item $s^{(m+d+1)}=0$.
\end{enumerate}
We also let $m'=m+d+1$ for writing simplicity. Note that each measurement is a $d$-dimensional vector, i.e. a point in the $d$-dimensional plane. Therefore, the Gaussian measurements are such that $[s^{(i)}]_n$ are non-correlated 0-mean $R^2$-variance gaussians for every $n\in[d]$. We then take a sample $v$ from the unit sphere and set $v^{(1)}=v$ and $v^{(2)}=2v$. Since again we are in dimension $d$, the sample $v$ is such that $v_1^2+\ldots+v_d^2=1$.
\paragraph{Tensor decomposition} We now construct a tensor $$\widetilde{F}\in\mathbb{C}^{m'\times m'\times 3}:\widetilde{F}_{n_1,n_2,n_3}=\widetilde{f}(s)|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}}$$ containing all the possible combinations of measurements from $\mathcal{S}\oplus\mathcal{S}\oplus\{v^{(1)},v^{(2)},0\}$, where $\oplus$ is the set addition operator. Note that there are $O((m')^2)$ such measurements. We apply an adaptation of Jennrich's algorithm on $\widetilde{F}$ (see Algorithm \ref{alg:adapt_Jennrich}) to obtain the estimates $\widehat{V}_{S'}$ and $\widehat{D}_w$. Note that the estimate $\widehat{V}_{S'}$ is normalised so that its last line is all ones, in order to simplify the decomposition.
\paragraph{Read of estimates} We finish by recovering the real part of the estimates of the point sources by setting $\widehat{\mu}^{(j)}=Real(\log([\widehat{V}_{S'}]_{[m+1:m+d,j]})/(i\pi))$ for every $j\in[k]$ and find the best possible matching weight coefficients by setting $\widehat{W}=\arg\min_{W\in\mathbb{C}^k}||\widehat{F}-\widehat{V}_{S'}\otimes\widehat{V}_{S'}\otimes \widehat{V}_dD_w||_F$, where $||\cdot||_F$ is the Frobenius norm of $(\cdot)$. Here, observe that by definition $[\widehat{V}_{S'}]_{[m+1:m+d,j]}=[\widehat{V}_d]_{[1:d,j]}$, where $\log(\widehat{V}_d)$ is performed element-wise in $\widehat{V}_d$ to recover the list of all estimates.
\begin{algorithm}
    \caption{Adaptation of Jennrich's algorithm}
    \label{alg:adapt_Jennrich}
    \begin{algorithmic}
        \State {\bf Input}: a tensor $\widetilde{F}\in\mathbb{C}^{m\times m\times 3}$ of rank $k$.
        \State Project $\widetilde{F}$ along the direction of the first basis vector.
        \State Compute the truncated SVD of the above projection that yields $\widetilde{F}(I,I,e_1)=\widehat{P}\widehat{\Lambda}\widehat{P}^T$.
        \State Set $\widehat{E}=\widetilde{F}(P,P,I)$ and set $\widehat{E}_i=\widehat{E}(I,I,e_i)$ for $i=1,2$.
        \State Let the columns of $\widehat{U}$ be the eigenvectors of $\widehat{E}_1\widehat{E}_2^{-1}$.
        \State {\bf Return} $\widehat{V}=\widehat{P}\widehat{U}$.
    \end{algorithmic}
\end{algorithm}
\subsection{Why we use tensor decomposition}
Jennrich's algorithm consists of a tensor decomposition that allows us to recover the estimates of the point sources. Namely, given a noisy tensor $\widetilde{F}$, it will spit out a decomposition that consists of three matrices, one of which we are most interested in. It is important to note that the crucical part in the adaptation of the original algorithm is the fact that we reduce the dimensionality of the original tensor in order to compute the decomposition faster. Indeed, the truncated SVD operation will allow us to recover the $k$ largest singular values such that the newly constructed tensor $E$ is of rank $k$ and its projection on $e_1$ becomes invertible. We can then apply the original algorithm on $E$ and spit out the desired decomposition.\par
We denote the following fact:
\begin{fact}
    In the exact case (i.e. when $\epsilon_z=0$), the constructed tensor $F$ admits a rank-$k$ decomposition $$F=V_{S'}\otimes V_{S'}\otimes (V_2D_w).$$
\end{fact}
\begin{proof}
    As usual, we start by computing the product $V_2D_w$. Since $D_w$ is a diagonal matrix, we have $$V_2D_w=
    \begin{pmatrix}
        w_1e^{i\pi\langle\mu^{(1)},v^{(1)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(1)}\rangle}\\
        w_1e^{i\pi\langle\mu^{(1)},v^{(2)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(2)}\rangle}\\
        w_1&\ldots&w_k
    \end{pmatrix}.$$
    Notice that once again, we can think of multiplying the last row of $w$'s with an exponential whose exponent is 0. Then, write $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}=e^{i\pi\langle\mu^{(c)},e_r=s(m+r)\rangle}$. Using this, we can express the $r$'th row and the $c$'th column of $V_{S'}$ as $$[V_{S'}]_{r,c}=e^{i\pi\langle\mu^{(c)},s^{(r)}\rangle},\quad r\in[m'],c\in[k].$$ From there for $n_1,n_2\in[m']$ and $n_3\in[3]$, it follows easily that 
    \begin{flalign*}
        F_{n_1,n_2,n_3}&=\sum_{n\in[k]}[V_{S'}]_{n_1,n}[V_{S'}]_{n_2,n}[V_2D_w]_{n_3,n}\\
        &=\sum_{n\in[k]}e^{i\pi\langle\mu^{(n)}, s^{(n_1)}\rangle}e^{i\pi\langle\mu^{(n)},s^{(n_2)}\rangle}w_ne^{i\pi\langle\mu^{(n)},v^{(n_3)}\rangle}\\
        &=\sum_{n\in[k]}w_ne^{i\pi\left(\langle\mu^{(n)},s^{(n_1)}\rangle+\langle\mu^{(n)},s^{(n_2)}\rangle+\langle\mu^{(n)},v^{(n_3)}\rangle\right)}\\
        &=\sum_{n\in[k]}w_ne^{i\pi\langle\mu^{(n)},s^{(n_1)}+s^{(n_2)}+v^{(n_3)}\rangle}\\
        &=\widetilde{f}(s)\big|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}},
    \end{flalign*} as required.
\end{proof}\par 
The above fact demonstrates why we use tensors. Indeed, having access to $\widehat{V}_{S'}$ allows us to recover the estimates of the point sources. The next sections are dedicated to the multiple stability guarantees that makes this algorithm both useful and powerful.

\section{Stability guarantees}
The main guarantee on the stability of the recovery of the point sources is given by $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq C\frac{\sqrt{d}k^5}{\Delta\delta_v}\frac{w_{max}}{w_{min}^2}\left(\frac{1+2\epsilon_x}{1-2\epsilon_x}\right)^{5/2}\epsilon_z,$$ where $C$ is a universal constant such that the bound holds with probability at least $(1-\delta_s)$ over the random sampling of $\mathcal{S}$ and with probability at least $(1-\delta_v)$ over the random projections in Jennrich's algorithm. We will see in the following section that the stability guarantees of the algorithm essentially rely on two important results: the stability of Jennrich's algorithm for tensor decomposition, and the tight variation bound that comes from the choice of the measurements.
\section{Analysis}
\subsection{Stability of tensor decomposition}
A main factor of stability resides in the tensor decomposition procedure. Since the measurements are linearly independent, the tensor decomposition exists and is unique up to column permutation and rescaling. The next few paragraphs go through steps that allow us to prove the stability guarantee that Jennrich's algorithm offers us.
\paragraph{Setup in the non-noisy case} Suppose that $F$ admits the decomposition $F=V\otimes V\otimes (V_2D_w)\in\mathbb{C}^{m\times m\times 3}$ and that $\widetilde{F}$ is element-wise close to $F$, that is, each of their elements differ by at most a constant $\epsilon_z$ in absolute value. If $\widetilde{F}$ is passed as input to Jennrich's algorithm, it happens that $\widehat{V}$ is close enough to $V$ with high probability. To show this, denote $D_1=diag([V_2]_{1,:}D_w)$ and $D_2=diag([V_2]_{2,:}D_w)$ where $[V_2]_{i,:}$ is the vector formed by the $i$'th row of $V_2$. Let $F_1=F(I,I,e_1)$. Applying claim \ref{thm:decomp}, we easily find out that 
\begin{flalign*}
    [F_1]_{n_1,n_2}=F(I,I,e_1)_{n_1,n_2}&=\sum_{j_1,j_2,j_3\in[m]}F_{j_1,j_2,j_3}[I]_{j_1,n_1}[I]_{j_2,i_2}[e_1]_{j_3}\\
    &=\sum_{n\in[k]}[V]_{n_1,n}[V]_{n_2,n}[V_2D_w]_{1,n}\\
    &=[VDiag([V_2]_{1,:}D_w)V^T]_{n_1,n_2},
\end{flalign*} which implies that the above projection can be decomposed as $F_1=VD_1V^T$. Now consider its SVD $F_1=P\Lambda P^T$. Additionally, define the whitened\footnote{A whitening transformation has the objective of linearly transforming a vector of correlated random variables into a new vector, the elements of which are uncorrelated. Here, the random elements that form the new tensor $E$ have as expected, zero correlation.} rank-$k$ tensor $E=F(P,P,I)$. Observe that again, by claim \ref{thm:decomp}, we can write $E$ as 
\begin{flalign*}
    E_{n_1,n_2,n_3}=F(P,P,I)_{n_1,n_2,n_3}&=\sum_{j_1,j_2\in[m']}\sum_{j_3\in[3]}F_{j_1,j_2,j_3}[P]_{j_1,n_1}[P]_{j_2,n_2}[I]_{j_3,n_3}\\
    &=\sum_{j_1,j_2\in[m']}\sum_{n\in[k]}[P^T]_{n_1,j_1}V_{j_1,n}[P^T]_{n_2,j_2}V_{j_2,n}[V_2D_w]_{n_3,n}\\
    &=\sum_{n\in[k]}[P^TV]_{n_1,n}[P^TV]_{n_2,n}[V_2D_w]_{n_3,n},
\end{flalign*} so that $E=(P^TV)\otimes(P^TV)\otimes(V_2D_w)=U\otimes U\otimes(V_2D_w)$ when writing $U:=P^TV\in\mathbb{C}^{k\times k}$.\par
Now slice the tensor $E$ in two such that we get its projections on the first two basis vectors of the plane. Formally, we have $E_1=E(I,I,e_1)$ and $E_2=E(I,I,e_2)$. Observe that $E_1=UD_1U^T$ and $E_2=UD_2U^T$. The proof for this is the same as that of $F_1$ and comes from the definition of linear mappings. Then, computing $E_1E_2^{-1}$ yields the following eigendecomposition:
\begin{flalign*}
    E_1E_2^{-1}&=UD_1U^T(U^TD_2U)^{-1}=UD_1U^T(U^T)^{-1}D_2^{-1}U^{-1}\\
    &=UD_1D_2^{-1}U^{-1}=UDU^{-1},
\end{flalign*} by setting $D=D_1D_2^{-1}$. Observe that in the exact case, $$D=diag(e^{i\pi\langle\mu^{(j)},v^{(1)}-v^{(2)}\rangle}:j\in[k]).$$
To show this, we first have to note that taking the inverse of a diagonal matrix only requires us to take the reciprocal of the diagonal elements and store them in a resulting matrix which is itself diagonal. Now recall that $[V_2D_w]_{r,c}=w_ce^{i\pi\langle\mu^{(c)},v^{(r)}\rangle}$. We can express $D_1$ and $D_2$ a little differently. Namely, write $[D_1]_{j,j}=w_je^{i\pi\langle\mu^{(j)},v^{(1)}\rangle}$ and $[D_2]_{j,j}=w_je^{i\pi\langle\mu^{(j)},v^{(2)}\rangle}$. Using the above fact allows us to express the inverse of $D_2$ using compact form as $[D_2^{-1}]_{j,j}=\frac{1}{w_j}e^{-i\pi\langle\mu^{(j)},v^{(2)}\rangle}$. Hence, $$[D]_{j,j}=[D_1D_2^{-1}]_{j,j}=e^{i\pi\langle\mu^{(j)},v^{(1)}-v^{(2)}\rangle},$$ as required.\par
We finish by letting $M:=E_1E_2^{-1}$. Note that since the columns of $U$ are set to be the eigenvectors of $M$, by definition it suffices to compute the product $PU$ to return our estimates since $PU=PP^TV=V.$ Hence, the decomposition works as expected.\par 
The above shows us why the dimensionality reduction is essential for our procedure. Indeed, tensor decomposition is in general a heavy operation, and it does not shy getting heavier as the tensor's dimensions grow. Here however, the decomposition is done on the smaller tensor $E$ that, thanks to the truncated SVD, keeps most of the important information that $F$ carries.
\paragraph{Proof sketch of stability} We can now start giving the general ideas of the proof that the above tensor decomposition procedure is stable.
\begin{enumerate}
    \item {\bf Noise affects the estimates mildly}: Jennrich's algorithm takes a large tensor as input and computes its decomposition. However, a key step in this decomposition is the creation of a lower dimension tensor that allows us to do the computations faster whilst keeping the stability guarantees. Here, matrix perturbation bounds show that the noise in the input tensor $\widetilde{F}$ propagates the estimates $\widehat{P}$ and $\widehat{E}$ in a mild way, as long as there is a tight enough upper bound on the condition number of $V$.
    \item {\bf Perturbation affects eigendecomposition mildly}: Suppose we write $\widehat{M}=\widehat{E}_1\widehat{E}_2^{-1}$ in terms of some perturbation matrices $H$ and $G$ defined as $H=-Z_2(I+E_2^{-1}Z_2)^{-1}E_2^{-1}$ and $G=Z_1E_2^{-1}$, where $Z_i$ is the additive noise in the estimates $\widehat{E}_i$ for $i=1,2$. We can use \textit{Gershgorin's disk theorem} to show that if the perturbations are negligible (i.e. $||H||$ and $||G||$ are small) and that the minimal separation $sep(D)$ between the diagonal entries of $D$ is large enough, then the eigendecomposition of $\widehat{M}$ is close to that of $M$.
    \item {\bf Bounds on the minimal separation and the perturbations}: One can use anti-concentration bounds on the random projection to show that $sep(D)$ is large enough, and from there, use the fact that $||\widehat{U}_j-U_j||_2$ is small to deduce tight enough bounds on $||H||$ and $||G||$.
    \item {\bf The estimates $\widehat{V}$ are close enough to $V$}: We can finally use (3.) along with perturbation bounds again to prove that the estimates $\widehat{V}_i=\widehat{P}\widehat{U}_i$ are close enough to the actual values returned in the non-noisy case and hence conclude on the confirmed stability of tensor decomposition.
\end{enumerate}
\subsection{Bounds on the condition numbers}
None of the above happens if the condition number of the characteristic matrix cannot be bounded. We describe here the general ideas that allow us to find a tight bound for $\cond(V_S)$.
\begin{enumerate}
    \item {\bf The condition numbers of $V_S$ and $V_{S'}$ are close enough}: Using the definition of the condition number of a matrix, it can easily be shown that $\cond(V_{S'})\leq\sqrt{1+\sqrt{k}}\cond(V_S)$.
    \item {\bf The eigenvalues of a random Gaussian Hermitian matrix are close to 1 in expectation}: Define $X_s\in\mathbb{C}^{k\times k}$ to be such that $[X_s]_{r,c}=e^{i\pi\langle\mu^{(c)}-\mu^{(r)},s\rangle}$. Then using the MGF of a Gaussian random variable, the assumption on the minimum distance of the point sources as well as geometric series, it can be shown that for any row of $Y:=\mathbb{E}_s[X_s]$, the sum of all its non-diagonal entries is never greater than $\epsilon_x$. From there, we can use Gershgorin's disk theorem to bound all the eigenvalues of $Y$ by $1\pm\epsilon_x$.
    \item {\bf The random sampling of $\mathcal{S}$ affects the condition number of $V_S$}: Since the procedure involves taking two samples from a Gaussian distribution, we can express $V_S^*V_S$ as the sample mean of the random Hermitian matrices $X^{(i)}$, namely $V_S^*V_S=\frac{1}{m}\sum_m X^{(m)}$. Hence using the previous result, since each element of $X_s$ lies on the unit circle in the complex plane and thus that $X_s^2\preceq k^2 I$ almost surely, it suffices to use the Matrix Hoeffding lemma to show that for some large enough $m$, the sum of each Gaussian concentrates around the mean of $X_s$ with high probability.
\end{enumerate}
The combination of all above results allows us to write that $$\cond(V_S)\leq\sqrt{\frac{1+2\epsilon_x}{1-2\epsilon_x}},$$ which is close to 1. Since this bound is tight enough, Jennrich's algorithm achieves stability and therefore, the recovery of the point sources is good enough.