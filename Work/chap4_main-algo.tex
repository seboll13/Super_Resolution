\chapter{Main algorithm}
Let us recall what the goal of the problem is. Broadly speaking, super-resolution aims to recover a superposition of point sources using bandlimited measurements that may be corrupted with noise. In this setting, the created algorithm works in the Fourier domain where each of the $k$ points of the $d$-dimensional plane are separated by a distance at least $\Delta$. Hence, the frequencies of the Fourier measurements are bounded by $O(1/\Delta)$. The idea of the procedure is to take random bandlimited measurements with cutoff frequency bounded by $\Omega(\sqrt{d}/\Delta)$ and perform Tensor decomposition to recover the estimates of the point sources.
\section{Algorithm description}
\paragraph{Input and output} The algorithm takes as input a cutoff frequency $R$, the defined number of measurements $m$ and a noisy measurement function $\tilde{f}(\cdot)$ and outputs the set of estimates $$\{\widehat{w}_j,\widehat{\mu}^{(j)}:j\in[k]\},$$ where the $\widehat{w}_j$'s are the complex weight coefficients and the $\widehat{\mu}^{(j)}$'s are the estimates of the point sources. Note that in case the noise is non-existant (i.e. when $\epsilon_z=0$), the parameters are recovered exactly. Otherwise, stable recovery implies that the estimates are such that $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq poly(d,k)\epsilon_z.$$ In words, the estimates of the point sources differ from their real values by at most the noise $\epsilon_z$ scaled by a polynomial function that depends on the number of dimensions $d$ and the number of point sources $k$.
\paragraph{Measurements} We first generate a set $\mathcal{S}=\{s^{(1)},\ldots,s^{(m+n+1)}\}$ where $s^{(1)},\ldots,s^{(m)}$ are $m$ i.i.d. samples from the Gaussian distribution $\mathcal{N}(0,R^2I_{d\times d})$, $s^{(m+n)}=e_n$ for all $n\in[d]$ and $s^{(m+n+1)}=0$ and we let $m'=m+d+1$. We then take a sample $v$ from the unit sphere and set $v^{(1)}=v$ and $v^{(2)}=2v$. Since we are in dimension $d$, the sample $v$ is such that $x_1^2+\ldots+x_d^2=1$. Finally, we construct a tensor $\tilde{F}\in\mathbb{C}^{m'\times m'\times 3}:\tilde{F}_{n_1,n_2,n_3}=\tilde{f}(s)|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}}$.
\paragraph{Tensor decomposition} We apply Jennrich's algorithm on $\tilde{F}$ to obtain the estimates $\widehat{V}_{S'}$ and $\widehat{D}_w$. We then normalise each value of $\widehat{V}_{S'}$ so that the last element of each column is 1.
\paragraph{Read of estimates} We finish by recovering the real part of the estimates of the point sources by setting $\widehat{\mu}^{(j)}=Real(\log([\widehat{V}_S]_{[m+1:m+d,j]})/(i\pi))$ for every $j\in[k]$ and we find the best possible matching coefficients by setting $\widehat{W}=\arg\min_{W\in\mathbb{C}^k}||\widehat{F}-\widehat{V}_{S'}\otimes\widehat{V}_{S'}\otimes \widehat{V}_dD_w||_F$, where $||\cdot||_F$ is the Frobenius norm of $(\cdot)$.
\section{Analysis}
First of all, note that the sample complexity is determined solely by the condition numbers of $F$ and $V_S$.