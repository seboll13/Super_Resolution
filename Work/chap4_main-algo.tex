\chapter{Main algorithm}
Let us recall what the goal of the problem is. Broadly speaking, super-resolution aims to recover a superposition of point sources using bandlimited measurements that may be corrupted with noise. In this setting, the created algorithm works in the Fourier domain where each of the $k$ points of the $d$-dimensional plane are separated by a distance at least $\Delta$. Hence, the frequencies of the Fourier measurements are bounded by $O(1/\Delta)$. The idea of the procedure is to take random bandlimited measurements with cutoff frequency bounded by $\Omega(\sqrt{d}/\Delta)$ and perform Tensor decomposition to recover the estimates of the point sources.\par 
This chapter discusses the main algorithm along with its analysis. We will go through all the important results and describe in detail their implication on the main procedure to effectively recover the point sources.
\section{Algorithm description}
\paragraph{Input and output} The algorithm takes as input a cutoff frequency $R$, the defined number of measurements $m$ and a noisy measurement function $\tilde{f}(\cdot)$ and outputs the set of estimates $$\{\widehat{w}_j,\widehat{\mu}^{(j)}:j\in[k]\},$$ where the $\widehat{w}_j$'s are the complex weight coefficients and the $\widehat{\mu}^{(j)}$'s are the estimates of the point sources. Note that in case the noise is non-existant (i.e. when $\epsilon_z=0$), the parameters are recovered exactly. Otherwise, stable recovery implies that the estimates are such that $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq poly(d,k)\epsilon_z.$$ In words, the estimates of the point sources differ from their real values by at most the noise $\epsilon_z$ scaled by a polynomial function that depends on the number of dimensions $d$ and the number of point sources $k$.
\paragraph{Measurements} We first generate a set $\mathcal{S}=\{s^{(1)},\ldots,s^{(m+n+1)}\}$ where $s^{(1)},\ldots,s^{(m)}$ are $m$ i.i.d. samples from the Gaussian distribution $\mathcal{N}(0,R^2I_{d\times d})$, $s^{(m+n)}=e_n$ for all $n\in[d]$ and $s^{(m+n+1)}=0$ and we let $m'=m+d+1$. We then take a sample $v$ from the unit sphere and set $v^{(1)}=v$ and $v^{(2)}=2v$. Since we are in dimension $d$, the sample $v$ is such that $x_1^2+\ldots+x_d^2=1$. Finally, we construct a tensor $\tilde{F}\in\mathbb{C}^{m'\times m'\times 3}:\tilde{F}_{n_1,n_2,n_3}=\tilde{f}(s)|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}}$.
\paragraph{Tensor decomposition} We apply Jennrich's algorithm on $\tilde{F}$ to obtain the estimates $\widehat{V}_{S'}$ and $\widehat{D}_w$. Note that the estimate $\widehat{V}_{S'}$ is normalised so that its last line is all ones, in order to match the definition of $V_{S'}$ (see next section).
\paragraph{Read of estimates} We finish by recovering the real part of the estimates of the point sources by setting $\widehat{\mu}^{(j)}=Real(\log([\widehat{V}_S]_{[m+1:m+d,j]})/(i\pi))$ for every $j\in[k]$ and we find the best possible matching coefficients by setting $\widehat{W}=\arg\min_{W\in\mathbb{C}^k}||\widehat{F}-\widehat{V}_{S'}\otimes\widehat{V}_{S'}\otimes \widehat{V}_dD_w||_F$, where $||\cdot||_F$ is the Frobenius norm of $(\cdot)$.
\section{Stability guarantee}
The main guarantee on the stability of the recovery of the point sources is given by $$\min_\pi\max\left\{||\widehat{\mu}^{(j)}-\mu^{(\pi(j))}||_2:j\in[k]\right\}\leq C\frac{\sqrt{d}k^5}{\Delta\delta_v}\frac{w_{max}}{w_{min}^2}\left(\frac{1+2\epsilon_x}{1-2\epsilon_x}\right)^{5/2}\epsilon_z,$$ where $C$ is a universal constant such that the bound holds with probability at least $(1-\delta_s)$ over the random sampling of $\mathcal{S}$ and with probability at least $(1-\delta_v)$ over the random projections in Jennrich's algorithm. Note that {\bf the spatial resolution of an imaging device may be measured by how closely lines can be resolved}.
\section{Analysis}
% First of all, note that the sample complexity is determined solely by the condition numbers of $F$ and $V_S$.
Let us first recall what variables are used in the main algorithm.
\begin{itemize}
    \item $[V_S]_{r,c}=e^{i\pi\langle\mu^{(c)},s^{(r)}\rangle}$ is the \textit{characteristic matrix}.
    \item $[V_d]_{r,c}=e^{i\pi\mu_r^{(c)}}$ is the matrix that contains all the complex valued coefficients that lie on the unit circle of the complex plane due to the uniformity of the $\mu$'s.
    \item $V_{S'}=\big[V_S,\, V_d,\, [1]^k\big]^T$ is the matrix that is part of the decomposition we are trying to compute.
    \item $$V_2=
    \begin{pmatrix}
        e^{i\pi\langle\mu^{(1)},v^{(1)}\rangle}&\ldots&e^{i\pi\langle\mu^{(k)},v^{(1)}\rangle}\\
        e^{i\pi\langle\mu^{(1)},v^{(2)}\rangle}&\ldots&e^{i\pi\langle\mu^{(k)},v^{(2)}\rangle}\\
        1&\ldots&1
    \end{pmatrix}.$$
\end{itemize}
\begin{fact}
    In the exact case (i.e. when $\epsilon_z=0$), the constructed tensor $F$ admits a rank-$k$ decomposition $$F=V_{S'}\otimes V_{S'}\otimes (V_2D_w).$$
\end{fact}
\begin{proof}
    As usual, we start by computing the product $V_2D_w$. Since $D_w$ is a diagonal matrix, we have $$V_2D_w=
    \begin{pmatrix}
        w_1e^{i\pi\langle\mu^{(1)},v^{(1)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(1)}\rangle}\\
        w_1e^{i\pi\langle\mu^{(1)},v^{(2)}\rangle}&\ldots&w_ke^{i\pi\langle\mu^{(k)},v^{(2)}\rangle}\\
        w_1&\ldots&w_k
    \end{pmatrix}.$$
    For simplicity of notation, define $v^{(3)}=0$ so that $e^{i\pi\langle\mu^{(j)},v^{(3)}\rangle}=1$ for every $j\in[k]$. Using this, we can express the $r$'th line and $c$'th column in $V_2D_w$ as $$[V_2D_w]_{r,c}=w_ce^{i\pi\langle\mu^{(c)},v^{(r)}\rangle},$$ for every $r\in[3]$ and $c\in[k]$. What we want to show is $$F_{n_1,n_2,n_3}=\sum_{n\in[k]}[V_{S'}]_{n_1,n}[V_{S'}]_{n_2,n}[V_2D_w]_{n_3,n},$$ where $F_{n_1,n_2,n_3}=\tilde{f}(s)\big|_{s=s^{(n_1)}+s^{(n_2)}+v^{(n_3)}}$ are the noisy measurements.
\end{proof}
\subsection{Exact case recovery}
