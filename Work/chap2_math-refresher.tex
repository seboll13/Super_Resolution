\chapter{Mathematical refresher}
\section{A mathematical theory of super-resolution}
\paragraph{Mathematical abstraction} We consider $k$ point sources in $d$ dimensions, where the points are separated by a distance at least $\Delta$ (in Euclidean distance). The $d$-dimensional signal $x(t)$ can be modeled as a weighted sum of $k$ Dirac measures in $\mathbb{R}^d$ as $$x(t)=\sum_{j=1}^k w_j\delta_{\mu^{(j)}},$$ where the $\mu^{(j)}$'s are the point sources in $\mathbb{R}^d$ and $w_j\in\mathbb{C}$ the weights such that $|w_j|<C$ for every $j\in[k]$ and some absolute constant $C>0$. $\Delta$ is by definition the minimal possible distance between any two pair of points of the $d$-dimensional plane. Formally, we regard $\Delta$ in terms of Euclidean distance as $$\Delta=\min_{j\neq j'}||\mu^{(j)}-\mu^{(j')}||_2.$$
\paragraph{Measurement function} There are two main factors that represent our signal: the weights $w_j$ and the complex low pass point spread functions $e^{i\pi\langle s,t\rangle}$ for some measurement $s$. The former is simply a weighting factor that is not too relevant to us here. The latter however represents the response of an imaging system to a point source. That is, it determines the overall performance level of our system and will be of crucial interest to our study.\par 
We define the measurement function $f(s):\mathbb{R}^d\to\mathbb{C}$ as being the convolution of the point source $x(t)$ with the point spread function $e^{i\pi\langle s,t\rangle}$. Formally, $$f(s)=\sum_{j\in[k]}w_je^{i\pi\langle\mu^{(j)},s\rangle}.$$ The function $f$ is in essence a sum of weighted Fourier coefficients. Note that the measurements $s$ in the noisy setting are corrupted with a uniformly bounded perturbation $z$ so that the noisy recovery problem becomes $$\widetilde{f}(s)=f(s)+z(s),$$ in which $|z(s)|\leq\epsilon_z$ for every $s$ and a constant $\epsilon_z\in(0,1/2)$. The idea is that given access to the signal $x(t)$, we wish to generate a set of random bandlimited Fourier measurements and evaluate the noisy function $\widetilde{f}$ on every measurement $s$ in order to recover the parameters $\{w_j,\mu^{(j)}:j\in[k]\}$ of the point source signal as best as we can. The problem statement will be described in more detail in Chapter 4.

\section{Main tools}
The main mathematical tools that are needed to understand the paper essentially lie around the subject of linear algebra. They include notions related to vectors, matrices and tensors. We also require some probabilistic analysis tools since the algorithm is partly random. In this chapter, we introduce those tools, prove the more important results and closely relate them to the paper.
\subsection{Generalised eigenvalue problem}
Before introducing the generalised eigenvalue problem, it is important to recall what a condition number is for a particular matrix. Suppose for instance we have a matrix $X\in\mathbb{R}^{m\times n}$. We let $\lambda_1,\ldots,\lambda_n$ be the eigenvalues of $X^TX$ (with repetitions) and arrange them so that $\lambda_1\geq\ldots\geq\lambda_n\geq 0$. Then, the $\sigma_1\geq\ldots\geq\sigma_n\geq 0$ such that $\sigma_i=\sqrt{\lambda_i}$ are called the \textit{singular values} of $X$. Define $\sigma_{max}(X)=\sigma_1$ and $\sigma_{min}(X)=\sigma_n$. We then define the condition number of a matrix to be the ratio between the largest and the smallest singular value of $X$. That is 
\begin{equation}
    \cond(X)=\sigma_1/\sigma_n=\frac{\sigma_{max}(X)}{\sigma_{min}(X)}.
\end{equation}
The above factor governs the noise tolerance of the generalised eigenvalue problem, i.e. it is the measure of sensitiveness of a matrix to arbitrary perturbations. Taking its limiting value will allow us to state whether or not the main algorithm achieves stable recovery of the point sources.\par
The goal of an eigenvalue problem is to simply find the eigenvalues of a particular matrix $A$. To do so, we generally solve the equation $AU=UV$, where $U$ is an orthogonal matrix. Multiplying each side by $U^T$ yields $$AUU^T=UVU^T\Rightarrow A=UVU^T=UVU^{-1},$$ where $\mathbb{R}^{d\times d}\ni U:=(u_1,\ldots,u_d)$ are the eigenvectors and $\mathbb{R}^{d\times d}\ni V:=Diag\left[(\lambda_1,\ldots,\lambda_d)^T\right]$ the eigenvalues. However, in the generalised version of this problem, we add another random matrix $B$ such that the problem becomes 
\begin{flalign*}
    AU=BUV&\Rightarrow AUU^T=BUVU^T\\
    &\Rightarrow A=BUVU^T=BUVU^{-1},
\end{flalign*}
where $U$ again are the eigenvectors and $V$ the eigenvalues. Note that $A$ and $B$ are both symmetric and that $B$ is generated randomly with the only condition being that it is positive definite. We form the pair $(A,B)$ called the \textit{pencil} and the pair $(U,V)$ called the \textit{eigenpair}. Therefore, we introduce a first version of what is called the matrix pencil method, in which given a pair of matrices $(A,B)$, we wish to find the generalised eigenvalues $\lambda$ for which there is a vector ${\bf x}$ such that $A{\bf x}=\lambda B{\bf x}$. Notice that the eigenvalues of $A$ are the solution to the generalised eigenvalue problem where $B=I$.
\subsection{Common norms}
\paragraph{Vectors} Norms are a powerful linear algebra tool that allows us to retreive important information regarding a particular matrix or vector. The most common vector norm is the $L^p$-norm defined for a vector $X$ of size $n$ as 
\begin{equation}
    ||X||_p=\left(\sum_{i\in[n]}x_i^p\right)^{1/p}.
\end{equation}
Note that $p$ can take any value in $\mathbb{N}^*$ but the most common ones are 1, 2 or $\infty$. In essence, the $L^1$-norm is the sum of absolute values of our vector, the $L^2$-norm represents the Euclidean distance of the vector from the origin and the $L^\infty$ norm is by definition the largest element in the vector, in absolute value.
\paragraph{Matrices} The same way we define a norm operator for vectors, we can define a similar operator for matrices, that will however have a slightly different meaning. Essentially, a matrix norm is a vector norm in a vector space whose elements are matrices. Let $A$ be a matrix of size $m\times n$. We define the following important norms for $A$:
\begin{flalign}
    ||A||_1&=\max_{j\in[n]}\sum_{i\in[m]}|A_{i,j}|\\
    ||A||_2&=\sqrt{\lambda_{max}(A^*A)}=\sigma_{max}(A)\label{eqn:spectral_norm}\\
    ||A||_\infty&=\max_{i\in[m]}\sum_{j\in[n]}|A_{i,j}|,
\end{flalign} where $\lambda_{max}(A^*A)$ is the largest eigenvalue of $A^*A$ in which $A^*$ denotes the conjugate transpose of $A$. Note that equality \ref{eqn:spectral_norm} is sometimes called the \textit{spectral norm} of a matrix. We also consider another important matrix norm called the \textit{Frobenius} norm, which is an equivalent version of the vector $L^2$-norm. It essentially represents the size of the matrix $A$, and we define it as 
\begin{equation}
    ||A||_F=\sqrt{\sum_{i\in[m]}\sum_{j\in[n]}A_{i,j}^2}.
\end{equation}
\subsection{Gershgorin's disk theorem}
A powerful result of linear algebra allows us to relate the eigenvalues of some matrix $A$ to points in the complex plane. Namely, the Gershgorin's disk theorem gives us a pretty good and efficient way to find the eigenvalues of a particular matrix by drawing disks in the complex plane and relating them to the eigenvalues. For the purpose, let $Y\in\mathbb{C}^{k\times k}$ be a complex valued matrix. For every $j\in[k]$, let $$\mathcal{D}_j=\big\{x\in\mathbb{C}:||x-Y_{j,j}||\leq\sum_{j'\neq j}|Y_{j,j'}|\big\}$$ be the disk associated with the $j$'th row of $Y$, defined on the complex plane. Then, Gershgorin's disk theorem states that all the eigenvalues of $Y$ must lie within the union of all the above defined disks. The intuition behind this theorem starts with the definition of an eigenvalue. Suppose for instance that $\lambda$ is an eigenvalue of $Y$. Then we know we can write $Yv=\lambda v$, where $v$ is an eigenvector of $Y$. Select $j$ to be the index whose entry has largest magnitude in $v$. Writing the matrix-vector product as a sum yields $[Yv]_j=\sum_{j'\in[n]}Y_{j,j'}v_{j'}=\lambda v_j$. We can split this sum into two distinct sums as $\sum_{j'\in[n]}Y_{j,j'}v_{j'}=Y_{j,j}v_j+\sum_{j'\neq j}Y_{j,j'}v_{j'}$. Putting the factors of $v_{j'}$ on the same side gives us $$\sum_{j'\neq j}Y_{j,j'}v_{j'}=(\lambda-Y_{j,j})v_j.$$ We can divide by $v_j$ on both sides and take the absolute value to obtain $$\bigg|\sum_{j'\neq j}Y_{j,j'}v_{j'}/v_j\bigg|=\big|\lambda-Y_{j,j}\big|.$$ Using the triangle inequality for absolute values, we get
\begin{flalign*}
    \big|\lambda-Y_{j,j}\big|&\leq\sum_{j'\neq j}\big|Y_{j,j'}v_{j'}/v_j\big|\\
    &\leq\sum_{j'\neq j}\big|Y_{j,j'}\big|\big|v_{j'}/v_j\big|\text{ since $|ab|=|a||b|$ for any $a,b$}\\
    &\leq\sum_{j'\neq j}\big|Y_{j,j'}\big|,
\end{flalign*} where in the last line we used the fact that $v_j$ has the largest magnitude in $v$ such that $v_j\geq v_{j'}$ for every $j$. Hence, the Gershgorin disk centered at some diagonal $j$ has radius always at most the sum of all non-diagonal entries of $Y$ of the same row $j$.
\subsection{Probabilistic inequalities}
Here we present the main probability bounds that are used to prove the correctness of the main algorithm. The first lemma is a generalisation of the Hoeffding bound to random matrices. It tells us that with high probability, the values of a random Hermitian matrix concentrate around the mean. The second considers the projection of a high dimensional random vector onto a lower dimension plane. Namely, suppose that we consider a random vector that lies on the $m$-dimensional unit sphere and another fixed vector. Then, the lemma states that with high probability, the absolute value of their dot product is large. We now state the aforementionned lemmas:
\begin{lemma} (Matrix Hoeffding)
    Let $\{X^{(1)},\ldots,X^{(m)}\}$ be a set of independent, random, Hermitian matrices of dimension $k\times k$, with identical distribution $X$. Assume that $E[X]$ is finite, and $X^2\preceq\sigma^2 I$ for some positive constant $\sigma$ almost surely. Then, for all $\epsilon\geq 0$, $$Pr\left(\left|\left|\frac{1}{m}\sum_{i=1}^m X^{(i)} - E[X]\right|\right|_2\geq\epsilon\right)\leq ke^{-\frac{m^2\epsilon^2}{8\sigma^2}}.$$
\end{lemma}
\begin{lemma} (Vector random projection)
    Let $a\in\mathbb{R}^m$ be a random vector distributed uniformly over the $m$-dimensional unit sphere and fix $v\in\mathbb{C}^m$. Then, for $\delta\in(0,1)$, $$Pr\left(|\langle a,v\rangle|\leq\frac{||v||_2}{\sqrt{em}}\delta\right)\leq\delta.$$
\end{lemma}
\begin{proof} (for vector random projection)
    We give an intuitive proof of the above lemma, which is mostly based on lemma 2.2 from \cite{proofJLLemma}. Let $X_1,\ldots,X_d$ be independent samples from a standard Gaussian distribution and let $Y=(X_1,\ldots,X_d)/||X||$ be a vector formed by the normalised Gaussians. It is straightforward to see that $Y$ is uniformly distributed on the $d$-dimensional sphere. Let now $Z\in\mathbb{R}^k$ be the projection of $Y$ onto the first $k$ coordinates and let $L=||Z||^2$. Observe that $E[L]=d/k$ for $d<k$. Then, it is true that 
    \begin{align}
        Pr\big(L\leq\beta k/d\big)\leq\beta e^{\frac{k}{2}(1-\beta)},\quad\beta<1\\
        Pr\big(L\geq\beta k/d\big)\leq\beta e^{\frac{k}{2}(1-\beta)},\quad\beta>1
    \end{align}
    Both bounds together tell us that the squared spectral norm of the projection is in fact fairly concentrated around its expectation. Now to apply the above to our lemma, set $L$ to be $|\langle a,v\rangle|^2$ so that the bound becomes $$Pr\left(|\langle a,v\rangle|\leq\frac{\sqrt{\beta}||v||_2}{\sqrt{m}}\right)\leq\beta e^{\frac{k}{2}(1-\beta)}.$$
    Setting $\beta=\delta^2/e$ yields 
    $$Pr\left(|\langle a,v\rangle|\leq\frac{||v||_2}{\sqrt{em}}\delta\right)\leq\frac{\delta^2}{e}e^{\frac{k}{2}(1-\delta^2/e)}.$$
    Hence to prove the bound, it suffices to show that 
    $\frac{\delta^2}{e}e^{\frac{k}{2}(1-\delta^2/e)}\leq\delta$ or equivalently $\delta e^{\frac{k}{2}(1-\delta^2/e)}\leq e.$
    Since by assumption $\delta<1$ and $1<e^{k(1+\delta^2/e)/2}$, the proof is completed.
\end{proof}

\section{Tensor decomposition}
A tensor is a generalisation of a matrix to more than two dimensions. We can think of a tensor as a point in $\mathbb{C}^{m_1\times\ldots\times m_p}$ where $p$ is the order of the tensor. Most of the time here, $p=3$ since three dimensions suffice for our analysis. Note that if $T$ is an order three tensor of dimensions $m\times n\times p$, we can view it as a collection of $p$ matrices of size $m\times n$ stacked on top of each other. For example, the entry $A_{i,j,k}$ of a 3-tensor $A$ will simply be the $(i,j)$'th entry of the $k$'th matrix \cite{tensorMethods}. Furthermore, we define the \textit{rank} of a tensor $T$ as the minimum $k$ such that we can write $T$ as the sum of rank one tensors.
\subsection{Canonical polyadic decomposition}
Canonical Polyadic (CP) decomposition (or \textit{tensor rank decomposition}) is a procedure for tensors equivalent to that of the SVD for matrices in which we wish to express a $p$-tensor as an outer product of $p$ vectors. We first recall that the outer product of two vectors $u\in\mathbb{C}^m$ and $v\in\mathbb{C}^n$ gives a matrix $M\in\mathbb{C}^{m\times n}$ such that $[M]_{r,c}=u_rv_c$ for $r\in[m]$ and $c\in[n]$. From there, suppose that we are working with a rank $k$ 3-tensor $V\in\mathbb{C}^{m\times n\times p}$. CP decomposition allows us to express $V$ as
\begin{equation}
    V=\sum_{n\in[k]}[A]_{:,n}\circ[B]_{:,n}\circ[C]_{:,n},    
\end{equation} where $\circ$ denotes the outer product, $A\in\mathbb{C}^{m\times k}$, $B\in\mathbb{C}^{n\times k}$, $C\in\mathbb{C}^{p\times k}$ and where $[M]_{:,n}$ denotes the vector formed by the $n$'th column of $M$. We then say that the tensor $V$ admits the decomposition $V=A\otimes B\otimes C$, where $\otimes$ denotes the tensor product operation. Equivalently, element-wise we have 
\begin{equation}
    V_{j_1,j_2,j_3}=\sum_{n\in[k]}A_{j_1,n}B_{j_2,n}C_{j_3,n},
\end{equation} for $j_1\in[m],j_2\in[n],j_3\in[p]$.
\subsection{Tensor decomposition as a multilinear mapping}
We can use an alternative definition of tensor decomposition using the notion of a multi-linear mapping. Namely, for given dimensions $m_A$, $m_B$, $m_C$, the mapping $V(\cdot,\cdot,\cdot):\mathbb{C}^{m\times m_A}\times\mathbb{C}^{m\times m_B}\times\mathbb{C}^{m\times m_C}\to\mathbb{C}^{m_A\times m_B\times m_C}$ is defined as: 
\begin{equation}
    \left[V(X_A,X_B,X_C)\right]_{i_1,i_2,i_3}=\sum_{j_1,j_2,j_3\in[m]}V_{j_1,j_2,j_3}[X_A]_{j_1,i_1}[X_B]_{j_2,i_2}[X_C]_{j_3,i_3}.
\end{equation}
We can verify that for a particular vector $a\in\mathbb{C}^m$, the projection $V(I,I,a)$ of $V$ along the 3rd dimension is $V(I,I,a)=ADiag(C^T a)B^T$ as long as $V$ admits a tensor decomposition $V=A\otimes B\otimes C$. Indeed,
\begin{flalign*}
    \left[V(I,I,a)\right]_{i_1,i_2}&=\sum_{j_1,j_2,j_3\in[m]}V_{j_1,j_2,j_3}[I]_{j_1,i_1}[I]_{j_2,i_2}[a]_{j_3}\\
    &=\sum_{j_1,j_2,j_3\in[m]}\sum_{n\in[k]} A_{j_1,n}B_{j_2,n}C_{j_3,n}[I]_{j_1,i_1}[I]_{j_2,i_2}[a]_{j_3}\\
    &=\sum_{j_3\in[m]}\sum_{n\in[k]} A_{i_1,n}B_{i_2,n}C_{j_3,n}[a]_{j_3}\\
    &=\underbrace{\sum_{n\in[k]}A_{i_1,n}B_{i_2,n}}_{=AB}\underbrace{\sum_{j_3\in[m]}C_{j_3,n}[a]_{j_3}}_{=Diag(C^T a)}\\
    &=\left[ADiag(C^T a)B^T\right]_{i_1,i_2}.
\end{flalign*} Since the above is true for every $i_1,i_2$, and we do not consider the $i_3$ coordinate since $a$ is a vector, we get the desired equality.\par 
In a more general case, we can consider the projection of the tensor $V$ on three $m$ column matrices $X$, $Y$ and $Z$. Again we suppose that $V$ admits the decomposition $A\otimes B\otimes C$. In that case, it is true that 
\begin{flalign*}
    [V(X,Y,Z)]_{i_1,i_2,i_3}&=\sum_{j_1,j_2,j_3\in[m]}V_{j_1,j_2,j_3}[X]_{j_1,i_1}[Y]_{j_2,i_2}[Z]_{j_3,i_3}\\
    &=\sum_{j_1,j_2,j_3\in[m]}\sum_{n\in[k]}A_{j_1,n}B_{j_2,n}C_{j_3,n}[X]_{j_1,i_1}[Y]_{j_2,i_2}[Z]_{j_3,i_3}\\
    &=\sum_{j_1,j_2,j_3\in[m]}\sum_{n\in[k]}[X^T]_{i_1,j_1}A_{j_1,n}[Y^T]_{i_2,j_2}B_{j_2,n}[Z^T]_{i_3,j_3}C_{j_3,n}\\
    &=\sum_{n\in[k]}[X^TA]_{i_1,n}[Y^TB]_{i_2,n}[Z^TC]_{i_3,n}\\
    &=[(X^TA)\otimes(Y^TB)\otimes(Z^TC)]_{i_1,i_2,i_3}.
\end{flalign*}
Hence, we conclude that $V(X,Y,Z)$ admits the decomposition $(X^TA)\otimes(Y^TB)\otimes(Z^TC)$ for any arbitrary matrices $X$, $Y$ and $Z$.
\subsection{Tensor Frobenius norm}
We can also extend the matrix norm definition to that of a tensor. However in our work, we only will need the equivalent Frobenius norm which will be useful in the main algorithm. For the purpose, let $V$ be a rank $k$ 3-tensor of dimensions $m\times n\times 3$. Then, the Frobenius norm of $V$ is simply defined as 
\begin{equation}
    ||V||_F=\sqrt{\sum_{i\in[m]}\sum_{j\in[n]}\sum_{l\in[3]}[V]_{i,j,l}^2}.
\end{equation}
\subsection{Jennrich's algorithm}
As previously stated, the goal of tensor decomposition is to, given a tensor $T$ of rank $k$, decompose it as a sum of rank 1 tensors of appropriate dimensions. Jennrich's algorithm is commonly used for tensor decomposition. We denote the following theorem:
\begin{theorem}
    Let $T=\sum_{j=1}^k u_i\otimes v_i\otimes w_i$ be a tensor in which each set of vectors $\{u_i\}_i$ and $\{v_i\}_i$ are linearly independent. Moreover, each pair of vectors in $\{w_i\}_i$ are also linearly independent. Then, the above decomposition is unique up to rescaling, and there is an efficient algorithm to find it.
\end{theorem}
The aforementioned algorithm is described in Algorithm \ref{alg:Jennrich}.
\begin{algorithm}
    \caption{Jennrich's algorithm for tensor decomposition}
    \label{alg:Jennrich}
    \begin{algorithmic}
        \State {\bf Input}: a tensor $\widetilde{F}\in\mathbb{C}^{m\times m\times 3}$ of rank $k$.
        \State Choose random unit vectors $a,b\in\mathbb{R}^m$.
        \State Compute $\widetilde{F}(I,I,a)=UD_aV^T$, where $D_a=Diag(\langle w^{(i)},a\rangle)$.
        \State Compute $\widetilde{F}(I,I,b)=UD_bV^T$, where $D_b=Diag(\langle w^{(i)},b\rangle)$.
        \State Compute the diagonalisations $\widetilde{F}(I,I,a)\widetilde{F}(I,I,b)^{-1}$ and $\widetilde{F}(I,I,b)\widetilde{F}(I,I,a)^{-1}$.
        \State Solve the linear system to recover the $w_j$'s.
        \State {\bf Return} $U$, $V$, $W$.
    \end{algorithmic}
\end{algorithm}\par
Observe that $$\widetilde{F}(I,I,a)\widetilde{F}(I,I,b)^{-1}=UD_aV^T(V^T)^{-1}D_b^{-1}U^{-1}=UD_aD_b^{-1}U^{-1},$$ and similarly $$\widetilde{F}(I,I,a)^{-1}\widetilde{F}(I,I,b)=(V^T)^{-1}D_a^{-1}U^{-1}UD_bV^T=(V^T)^{-1}D_a^{-1}D_bV^T.$$ The correctness of the algorithm follows from the uniqueness of eigendecomposition of a matrix when the eigenvalues are distinct. For a random choice of $a$ and $b$ (in our case we choose the basis vectors $e_1$ and $e_2$), with high probability the eigendecompositions are unique so we can recover the $u_i$'s and the $v_i$'s easily by simply recovering the columns of $U$ and $V$, respectively.