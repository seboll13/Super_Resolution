\chapter{Mathematical refresher}
\section{A mathematical theory of super-resolution}
We consider $k$ point sources in $d$ dimensions, where the points are separated by a distance at least $\Delta$ (in Euclidean distance). The $d$-dimensional signal $x(t)$ can be modeled as a weighted sum of $k$ Dirac measures in $\mathbb{R}^d$ as $$x(t)=\sum_{j=1}^k w_j\delta_{\mu^{(j)}},$$ where the $\mu^{(j)}$'s are the point sources in $\mathbb{R}^d$ and $w_j\in\mathbb{C}$ the weights such that $|w_j|<C$ for every $j\in[k]$ and some absolute constant $C>0$.

\section{Main tools}
The main mathematical tools that are needed to understand the paper essentially lie around the subject of linear algebra. They include operations related to vectors, matrices and tensors. We also require some probabilistic analysis tools since the algorithm is partly random. In this chapter, we introduce those tools, prove the more important results and closely relate them to the paper. 
\subsection{Generalised eigenvalue problem}
Before introducing the generalised eigenvalue problem, it is important to recall what a condition number is for a particular matrix. Suppose for instance we have a matrix $X\in\mathbb{R}^{m\times n}$. We let $\lambda_1,\ldots,\lambda_n$ be the eigenvalues of $X^TX$ (with repetitions) and arrange them so that $\lambda_1\geq\ldots\geq\lambda_n\geq 0$. Then, the $\sigma_1\geq\ldots\geq\sigma_n\geq 0$ such that $\sigma_i=\sqrt{\lambda_i}$ are called the \textit{singular values} of $X$. Define $\sigma_{max}(X)=\sigma_1$ and $\sigma_{min}(X)=\sigma_n$. We then define the condition number of a matrix to be the ratio between the largest and the smallest singular value of $X$. That is 
\begin{equation}
    \cond(X)=\sigma_1/\sigma_n=\frac{\sigma_{max}(X)}{\sigma_{min}(X)}.
\end{equation}
The above factor governs the noise tolerance of the generalised eigenvalue problem, i.e. it is the measure of sensitiveness of a matrix to arbitrary perturbations. Taking its limiting value will allow us to state whether or not the main algorithm achieves stable recovery of the point sources.\par
The goal of an eigenvalue problem is to simply find the eigenvalues of a particular matrix $A$. To do so, we generally solve the following equation: $AU=UV\Rightarrow AUU^T=UVU^T\Rightarrow A=UVU^T=UVU^{-1}$, where $\mathbb{R}^{d\times d}\ni U:=(u_1,\ldots,u_d)$ are the eigenvectors and $\mathbb{R}^{d\times d}\ni V:=Diag\left[(\lambda_1,\ldots,\lambda_d)^T\right]$ the eigenvalues. However, in the generalised version of this problem, we add another random matrix $B$ such that the problem becomes $$AU=BUV\Rightarrow AUU^T=BUVU^T\Rightarrow A=BUVU^T=BUVU^{-1},$$ where $U$ again are the eigenvectors and $V$ the eigenvalues. Note that $A$ and $B$ are both symmetric. We form the pair $(A,B)$ called the \textit{pencil} and the pair $(U,V)$ called the \textit{eigenpair}. Therefore, we introduce a first version of what is called the matrix pencil method, in which given a pair of matrices $(A,B)$, we wish to find the generalised eigenvalues $\lambda$ for which there is a vector ${\bf x}$ such that $A{\bf x}=\lambda B{\bf x}$. Note that the eigenvalues of $A$ are the solution to the generalised eigenvalue problem where $B=I$.

\section{Tensor decomposition}
\subsection{Gentle introduction}
A tensor is a generalisation of a matrix to more than two dimensions. We can think of a tensor as a point in $\mathbb{C}^{m_1\times\ldots\times m_k}$ where $k$ is the order of the tensor. Most of the time here, $k=3$ since three dimensions suffice for our analysis. Note that if $T$ is an order three tensor of dimensions $m_A\times m_B\times m_C$, we can view it as a collection of $m_C$ matrices of size $m_A\times m_B$ stacked on top of each other. For example, the entry $A_{i,j,k}$ of a 3-tensor $A$ will simply be the $(i,j)$'th entry of the $k$'th matrix.\par
We define the \textit{rank} of a tensor $V$ as the minimum $r$ such that we can write $V$ as the sum of rank one tensors. A rank one tensor will be decomposed in the form of a tensor product of three matrices $A$, $B$ and $C$ as $V=A\otimes B\otimes C$. The above product is element-wise defined as $V_{i_1,i_2,i_3}=\sum_{j=1}^k A_{i_1,j}B_{i_2,j}C_{i_3,j}$.\par 
An alternative definition is given using the notion of a multi-linear mapping. Namely, for given dimensions $m_A$, $m_B$, $m_C$, the mapping $V(\cdot,\cdot,\cdot):\mathbb{C}^{m\times m_A}\times\mathbb{C}^{m\times m_B}\times\mathbb{C}^{m\times m_C}\to\mathbb{C}^{m_A\times m_B\times m_C}$ is defined as: $$\left[V(X_A,X_B,X_C)\right]_{i_1,i_2,i_3}=\sum_{j_1,j_2,j_3\in[m]}V_{j_1,j_2,j_3}[X_A]_{j_1,i_1}[X_B]_{j_2,i_2}[X_C]_{j_3,i_3}.$$
We can verify that for a particular vector $a\in\mathbb{C}^m$, the projection $V(I,I,a)$ of $V$ along the 3rd dimension is $V(I,I,a)=ADiag(C^T a)B^T$ as long as $V$ admits a tensor decomposition $V=A\otimes B\otimes C$. Indeed,
\begin{flalign*}
    \left[V(I,I,a)\right]_{i_1,i_2,i_3}&=\sum_{j_1,j_2,j_3\in[m]}V_{j_1,j_2,j_3}[I]_{j_1,i_1}[I]_{j_2,i_2}[a]_{j_3,i_3}\\
    &=\sum_{j_1,j_2,j_3\in[m]}\sum_{n\in[k]} A_{j_1,n}B_{j_2,n}C_{j_3,n}[I]_{j_1,i_1}[I]_{j_2,i_2}[a]_{j_3,i_3}\\
    &=...\\
    &=Diag(C^T a)(A\otimes B)\\
    &=ADiag(C^T a)B^T
\end{flalign*}
\subsection{Jennrich's algorithm}
As previously stated, the goal of tensor decomposition is to, given a tensor $T$ of rank $k$, decompose it as a sum of rank 1 tensors of appropriate dimensions. Jennrich's algorithm is commonly used for tensor decomposition. We denote the following theorem:
\begin{theorem}
    Let $T=\sum_{j=1}^k u_i\otimes v_i\otimes w_i$ be a tensor in which each set of vectors $\{u_i\}_i$ and $\{v_i\}_i$ are linearly independent. Moreover, each pair of vectors in $\{w_i\}_i$ are also linearly independent. Then, the above decomposition is unique up to rescaling, and there is an efficient algorithm to find it.
\end{theorem}
The aforementioned algorithm is described in Algorithm \ref{alg:Jennrich}.
\begin{algorithm}
    \caption{Jennrich's algorithm for tensor decomposition}
    \label{alg:Jennrich}
    \begin{algorithmic}
        \State {\bf Input}: a tensor $\tilde{F}\in\mathbb{C}^{m\times m\times 3}$ of rank $k$.
        \State Choose random unit vectors $a,b\in\mathbb{R}^m$.
        \State Compute $\tilde{F}(I,I,a)=UD_aV^T$, where $D_a=Diag(\langle w^{(i)},a\rangle)$.
        \State Compute $\tilde{F}(I,I,b)=UD_bV^T$, where $D_b=Diag(\langle w^{(i)},b\rangle)$.
        \State Compute the diagonalisations $\tilde{F}(I,I,a)\tilde{F}(I,I,b)^{-1}$ and $\tilde{F}(I,I,b)\tilde{F}(I,I,a)^{-1}$.
        \State Solve the linear system to recover the $w_j$'s.
        \State {\bf Return} $U$, $V$, $W$.
    \end{algorithmic}
\end{algorithm}
Observe that $$\tilde{F}(I,I,a)\tilde{F}(I,I,b)^{-1}=UD_aV^T(V^T)^{-1}D_b^{-1}U^{-1}=UD_aD_b^{-1}U^{-1},$$ and similarly $$\tilde{F}(I,I,a)^{-1}\tilde{F}(I,I,b)=(V^T)^{-1}D_a^{-1}U^{-1}UD_bV^T=(V^T)^{-1}D_a^{-1}D_bV^T.$$ The correctness of the algorithm follows from the uniqueness of eigendecomposition of a matrix when the eigenvalues are distinct. For a random choice of $a$ and $b$ (in our case we choose the basis vectors $e_1$ and $e_2$), with high probability the eigendecompositions are unique so we can recover the $u_i$'s and the $v_i$'s easily by simply recovering the columns of $U$ and $V$, respectively.